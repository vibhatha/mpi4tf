# -*- coding: utf-8 -*-
"""tensorflow_mnist_gradient_tape.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vtXQnFKliVldGm6uj3qhZGn5ize_Ecy9
"""

# !pip install tensorflow-gpu==2.0.0b1

# Commented out IPython magic to ensure Python compatibility.
from __future__ import absolute_import, division, print_function, unicode_literals

import tensorflow as tf

tf.executing_eagerly()

(mnist_train_images, mnist_train_labels), ((mnist_test_images, mnist_test_labels)) = tf.keras.datasets.mnist.load_data()

print(mnist_train_images.shape, mnist_train_labels.shape, mnist_test_images.shape, mnist_test_labels.shape)

mini_batch_size = 8

mnist_train_images = mnist_train_images[0:15000]
mnist_train_labels = mnist_train_labels[0:15000]

dataset = tf.data.Dataset.from_tensor_slices(
    (tf.cast(mnist_train_images[..., tf.newaxis] / 255, tf.float32),
     tf.cast(mnist_train_labels, tf.int64)))
dataset = dataset.shuffle(1000).batch(mini_batch_size)

# Build the model
mnist_model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(16, [3, 3], activation='relu',
                           input_shape=(None, None, 1)),
    tf.keras.layers.Conv2D(16, [3, 3], activation='relu'),
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dense(10)
])

for images, labels in dataset.take(1):
    print("Logits: ", mnist_model(images[0:1]).numpy())

optimizer = tf.keras.optimizers.Adam()
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

loss_history = []

def train_step(images, labels):
    with tf.GradientTape() as tape:
        logits = mnist_model(images, training=True)

        # Add asserts to check the shape of the output.
        tf.debugging.assert_equal(logits.shape, (mini_batch_size, 10))

        loss_value = loss_object(labels, logits)

    loss_history.append(loss_value.numpy().mean())
    grads = tape.gradient(loss_value, mnist_model.trainable_variables)
    optimizer.apply_gradients(zip(grads, mnist_model.trainable_variables))


def train(epochs):
    for epoch in range(epochs):
        for (batch, (images, labels)) in enumerate(dataset):
            train_step(images, labels)
        print('Epoch {} finished'.format(epoch))


import time

t1 = time.time()
with tf.device('/cpu:0'):
    train(epochs=1)
print("CPU TIME : {}".format(time.time() - t1))

# import matplotlib
#
# matplotlib.use('TkAgg')
# import matplotlib.pyplot as plt
#
# plt.plot(loss_history)
# plt.xlabel('Batch #')
# plt.ylabel('Loss [entropy]')
# plt.show()
